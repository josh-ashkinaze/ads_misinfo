{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b553e1ae",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652e3c06-dc49-4506-a769-cda44b90ad11",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install joblib numpy statsmodels scipy seaborn matplotlib\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "import seaborn as sns\n",
    "import random \n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b487aeba",
   "metadata": {},
   "source": [
    "# Power Analysis\n",
    "\n",
    "## Treatment Effect\n",
    "- A laboratory experiment (Kaiser et al (2022)) asked users to imagine that somebody the participant followed posted misinformation. Then the study asked participant's to rate their intentions to unfollow that individual on a scale of 1-6, with 1 being unfollowing \"does not at all apply\" and 6 being \"does fully apply\". Summing the percentage of respondents who answered 4-6 is a rough measure of the percentage of respondent's who would be more likely than not to unfollow. That sum is ~19%.\n",
    "\n",
    "- One study (Lin et al 2024) compared the effect size of accuracy nudges in the lab to the effect size of accurarcy nudges when used in a digital ad experiment similar to ours. We use this ratio to inform how much to discount Kaiser et al. (2022). From page 11, we use the lower bound of the ad efficacy (2.6%) and the upper bound of the lab estimate (10%) to arrive at a base discount factor of (1 - 2.6/10) 74%. Doing so would yield 26% of 19%  = 5%\n",
    "\n",
    "## Control Rate\n",
    "- Ashkinaze et al (2024) looked at the unfollowing rate of health misinformation spreaders and found it was 0.52% per month\n",
    "- We perturn this amount by +- 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8928f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 DAYS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Power Analysis:   0%|                                                                                  | 0/3990 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/statsmodels/stats/weightstats.py:792: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  zstat = value / std\n",
      "Running Power Analysis:   8%|█████▍                                                                  | 304/3990 [00:22<04:37, 13.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 137\u001b[0m\n\u001b[1;32m    127\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreat_status_MINIMAL_FOLLOWERS_03.04.2024__17.11.03__START0_END-1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol_prop\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.058\u001b[39m],\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment_prop\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreat_eligible\u001b[39m\u001b[38;5;124m\"\u001b[39m:[\u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m    135\u001b[0m }\n\u001b[0;32m--> 137\u001b[0m df_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_power_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m make_graph(df_results)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(find_required_N(df_results))\n",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m, in \u001b[0;36mrun_power_analysis\u001b[0;34m(params, df)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m N \u001b[38;5;129;01min\u001b[39;00m N_values:\n\u001b[1;32m     55\u001b[0m     adj_control_prop \u001b[38;5;241m=\u001b[39m control_prop \u001b[38;5;241m*\u001b[39m (n_days \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     power \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_control_prop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreatment_prop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreat_eligible\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_simulations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol_prop\u001b[39m\u001b[38;5;124m'\u001b[39m: control_prop, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreatment_prop\u001b[39m\u001b[38;5;124m'\u001b[39m: treatment_prop, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_days\u001b[39m\u001b[38;5;124m'\u001b[39m: n_days, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m: N, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m'\u001b[39m: power})\n\u001b[1;32m     58\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Update the progress bar after each iteration\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mestimate_power\u001b[0;34m(df, N, control_prop, treatment_prop, treat_eligible, alpha, n_simulations)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the power of the test, considering unequal sample sizes from 'df'.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Calculate N_control and N_treatment based on 'treated' column\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m N_control \u001b[38;5;241m=\u001b[39m df_sample[df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreated\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m N_treatment \u001b[38;5;241m=\u001b[39m df_sample[df_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtreated\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/generic.py:6115\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6115\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(416)\n",
    "random.seed(416)\n",
    "\n",
    "#########################\n",
    "#########################\n",
    "# # Power Analysis\n",
    "\n",
    "# ## Treatment Effect\n",
    "# - A laboratory experiment (Kaiser et al (2022)) asked users to imagine that somebody the participant followed posted misinformation. Then the study asked participant's to rate their intentions to unfollow that individual on a scale of 1-6, with 1 being unfollowing \"does not at all apply\" and 6 being \"does fully apply\". Summing the percentage of respondents who answered 4-6 is a rough measure of the percentage of respondent's who would be more likely than not to unfollow. That sum is ~19%. We then 'discount' this effect size by different values\n",
    "\n",
    "# - One study (Lin et al 2024) compared the effect size of accuracy nudges in the lab to the effect size of accurarcy nudges when used in a digital ad experiment similar to ours. We use this ratio to inform how much to discount Kaiser et al. (2022). From page 11, we use the lower bound of the ad efficacy (2.6%) and the upper bound of the lab estimate (10%) to arrive at a base discount factor of (1 - 2.6/10) 74%. That is, we assume the true effect of Kaiser et al (2022) will be roughly 26% of the lab effect. To simulate some uncertanity we randomly perturb the discount factor by +- 20%.\n",
    "\n",
    "# ## Control Rate\n",
    "# - Ashkinaze et al (2024) looked at the unfollowing rate of health misinformation spreaders and found it was 0.52% per month\n",
    "# - We perturb this amount by +- 20%\n",
    "\n",
    "#########################\n",
    "#########################\n",
    "# ASSUMPTIONS\n",
    "#########################\n",
    "#########################\n",
    "\n",
    "# Constants \n",
    "NOISE = 0.2\n",
    "N_MONTHS = 1\n",
    "\n",
    "# Treatment assumptions\n",
    "TREATMENT_FX = 0.19\n",
    "BASE_TREATMENT_DISCOUNT = 1 - (2.6/10)\n",
    "TREATMENT_DISCOUNT_RANGE = [(1-NOISE)*BASE_TREATMENT_DISCOUNT, (1+NOISE)*BASE_TREATMENT_DISCOUNT]\n",
    "\n",
    "# Control assumptions\n",
    "CONTROL_MONTHLY_UNFOLLOW = 0.0052\n",
    "CONTROL_MONTHLY_UNFOLLOW_RANGE = [(1-NOISE)*CONTROL_MONTHLY_UNFOLLOW, (1+NOISE)*CONTROL_MONTHLY_UNFOLLOW]\n",
    "\n",
    "def run_power_analysis(params, df):\n",
    "    \"\"\"Run power analysis and iterate over control_prop, treatment_prop, and N_values.\"\"\"\n",
    "    n_days_list = params['n_days']\n",
    "    control_props = params['control_prop']\n",
    "    treatment_props = params['treatment_prop']\n",
    "    N_values = params['N_values']\n",
    "    n_simulations = params['n_simulations']\n",
    "    treat_eligible = params['treat_eligible'][0]  # Assuming treat_eligible is a list with a single value\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Create a total iterations count for the tqdm progress bar\n",
    "    total_iterations = len(n_days_list) * len(control_props) * len(treatment_props) * len(N_values)\n",
    "    \n",
    "    with tqdm(total=total_iterations, desc=\"Running Power Analysis\") as pbar:\n",
    "        for n_days in n_days_list:\n",
    "            for control_prop in control_props:\n",
    "                for treatment_prop in treatment_props:\n",
    "                    for N in N_values:\n",
    "                        adj_control_prop = control_prop * (n_days / 30)\n",
    "                        power = estimate_power(df, N, adj_control_prop, treatment_prop, treat_eligible, n_simulations)\n",
    "                        results.append({'control_prop': control_prop, 'treatment_prop': treatment_prop, 'n_days': n_days, 'N': N, 'power': power})\n",
    "                        pbar.update(1)  # Update the progress bar after each iteration\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def estimate_power(df, N, control_prop, treatment_prop, treat_eligible, alpha=0.05, n_simulations=100):\n",
    "    \"\"\"Estimate the power of the test, considering unequal sample sizes from 'df'.\"\"\"\n",
    "    # Calculate N_control and N_treatment based on 'treated' column\n",
    "    df_sample = df.sample(N)\n",
    "    N_control = df_sample[df_sample['treated'] == 0].shape[0]\n",
    "    N_treatment = df_sample[df_sample['treated'] == 1].shape[0]\n",
    "    N_effective_treatment = treat_eligible * N_treatment\n",
    "    \n",
    "    significant_count = 0\n",
    "    for _ in range(n_simulations):\n",
    "        is_significant = run_simulation(N_control, N_effective_treatment, control_prop, treatment_prop, alpha)\n",
    "        if is_significant:\n",
    "            significant_count += 1\n",
    "    return significant_count / n_simulations\n",
    "\n",
    "def run_simulation(N_control, N_treatment, control_prop, treatment_prop, alpha):\n",
    "    \"\"\"Adjust to use N_control and N_treatment for z-test.\"\"\"\n",
    "    p_value = difference_in_proportions_test(N_control, N_treatment, control_prop, treatment_prop)\n",
    "    return p_value <= alpha\n",
    "    \n",
    "def make_graph(df_results):\n",
    "    df_results['treatment_prop_label'] = df_results['treatment_prop'].apply(lambda x: f\"{x:.3f}\")\n",
    "    plt.figure(figsize=(10,6))\n",
    "    title = r\"$\\bf{McNemar\\ Test\\ Power\\ Analysis}$\" + \"\\n\\nThe treatment unfollowing rate is assumed to be\\n30%, 20%, 10%, 7.5%, and 5% of the effect size\\nfrom Kaiser et al. (2022).\"\n",
    "    plt.title(title, ha='left', x=0)\n",
    "    sns.lineplot(data=df_results, x=\"N\", y=\"power\", hue=\"treatment_prop_label\")\n",
    "    plt.axhline(y=0.8, linestyle='dashed', color='red', linewidth=7)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Treatment Unfollowing Prop.\")\n",
    "    plt.xlabel(\"Number of (Treatment, Control) Pairs\")\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "    plt.savefig(\"power_analysis.png\", dpi=400)\n",
    "\n",
    "def find_required_N(df_results):\n",
    "    \"\"\"Find the N required to achieve 80% power for each treatment_prop.\"\"\"\n",
    "    df_filtered = df_results[df_results['power'] >= 0.8]\n",
    "    df_required_N = df_filtered.groupby('treatment_prop', as_index=False)['N'].min()\n",
    "    df_required_N['treatment_prop_label'] = df_required_N['treatment_prop'].apply(lambda x: f\"{x:.3f}\")\n",
    "    return df_required_N\n",
    "\n",
    "\n",
    "def difference_in_proportions_test(N_control, N_treatment, control_prop, treatment_prop):\n",
    "    \"\"\"\n",
    "    Calculate z-statistic and p-value for the difference in proportions\n",
    "    with unequal sample sizes using statsmodels.\n",
    "    \"\"\"\n",
    "    success_control = np.random.binomial(N_control, control_prop)\n",
    "    success_treatment = np.random.binomial(N_treatment, treatment_prop)\n",
    "    count = np.array([success_control, success_treatment])\n",
    "    nobs = np.array([N_control, N_treatment])\n",
    "\n",
    "    # Ensure that count and nobs are appropriate for proportions_ztest,\n",
    "    # which expects 1D arrays for count and nobs.\n",
    "    z_stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"14 DAYS\")\n",
    "\n",
    "df = pd.read_csv(\"treat_status_MINIMAL_FOLLOWERS_03.04.2024__17.11.03__START0_END-1.csv\")\n",
    "params = {\n",
    "    'control_prop': [0.058],\n",
    "    'treatment_prop': np.linspace(0.005, 0.05, 10),\n",
    "    'N_values': range(50, 20000, 50),\n",
    "    'n_days':[14],\n",
    "    'n_simulations': 1,\n",
    "    \"treat_eligible\":[0.3]\n",
    "}\n",
    "\n",
    "df_results = run_power_analysis(params, df)\n",
    "make_graph(df_results)\n",
    "print(find_required_N(df_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f4d629-0765-4648-8e22-6f62daa2de6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
